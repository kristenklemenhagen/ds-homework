{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Mental Health in the Technology Workplace: Predictors of Mental Health Disorders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preparation of data from previous notebook\n",
    "\n",
    "I received some excellent information from Phillippa about using pickle and helper functions to cut down on repeating the data cleaning steps in this notebook but did not have time to implement them here. I will definitely use them in the future, though. For now, the previous data cleaning and preparation is duplicated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "df=pd.read_csv('Mental-Health-in-Tech-Survey-2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# age needs cleaning - code adapted from a Kaggle kernel to get rid of low and high entries and convert to float\n",
    "df['age']=pd.to_numeric(df['age'],errors='coerce')\n",
    "def age_process(age):\n",
    "    if age>=18 and age<=100:\n",
    "        return age\n",
    "    else:\n",
    "        return np.nan\n",
    "df['age']=df['age'].apply(age_process)\n",
    "count_nan = len(df['age']) - df['age'].count()\n",
    "print count_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows where age was out of range\n",
    "df.dropna(subset=['age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'male ', 'female', 'm', 'i identify as female.', 'female ',\n",
       "       'bigender', 'non-binary', 'female assigned at birth ', 'f', 'woman',\n",
       "       'man', 'fm', 'cis female ', 'transitioned m2f',\n",
       "       'genderfluid (born female)', 'other/transfeminine',\n",
       "       'female or multi-gender femme', 'female/woman', 'cis male', 'male.',\n",
       "       'androgynous', 'male 9:1 female roughly', nan, 'male (cis)',\n",
       "       'other', 'nb masculine', 'cisgender female', 'sex is male',\n",
       "       'none of your business', 'genderqueer', 'human', 'genderfluid',\n",
       "       'enby', 'malr', 'genderqueer woman', 'mtf', 'queer', 'agender',\n",
       "       'dude', 'fluid',\n",
       "       'im a man why didnt you make this a drop down question. you should of asked sex? and i would of answered yes please. seriously how much text can this take? ',\n",
       "       'mail', 'm|', 'male/genderqueer', 'fem', 'nonbinary',\n",
       "       'female (props for making this a freeform field though)', ' female',\n",
       "       'unicorn', 'male (trans ftm)', 'cis-woman', 'cisdude',\n",
       "       'genderflux demi-girl', 'female-bodied; no feelings about gender',\n",
       "       'cis man', 'afab', 'transgender woman'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gender needs cleaning - was free text entry, so includes many misspellings and rare non-binary types\n",
    "# do not want to exclude non-binary, but not enough representatives to make their own categories\n",
    "# so make male, female, and nonbinary\n",
    "df.gender = df.gender.str.lower()\n",
    "df['gender'] = df['gender'].str.replace(r\"[\\\"\\',]\", '')\n",
    "df.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'nonbinary', nan], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping non-binary, spelling error, and other entries\n",
    "df.gender = df.gender.replace('male ', 'male')\n",
    "df.gender = df.gender.replace('m', 'male')\n",
    "df.gender = df.gender.replace('Male ', 'male')\n",
    "df.gender = df.gender.replace('i identify as female.', 'female')\n",
    "df.gender = df.gender.replace('female ', 'female')\n",
    "df.gender = df.gender.replace('bigender', 'nonbinary')\n",
    "df.gender = df.gender.replace('non-binary', 'nonbinary')\n",
    "df.gender = df.gender.replace('female assigned at birth ', 'nonbinary')\n",
    "df.gender = df.gender.replace('f', 'female')\n",
    "df.gender = df.gender.replace('woman', 'female')\n",
    "df.gender = df.gender.replace('man', 'male')\n",
    "df.gender = df.gender.replace('fm', 'female') # ???\n",
    "df.gender = df.gender.replace('cis female ', 'female')\n",
    "df.gender = df.gender.replace('transitioned m2f', 'female')\n",
    "df.gender = df.gender.replace('genderfluid (born female)', 'nonbinary')\n",
    "df.gender = df.gender.replace('other/transfeminine', 'nonbinary')\n",
    "df.gender = df.gender.replace('female or multi-gender femme', 'nonbinary')\n",
    "df.gender = df.gender.replace('female/woman', 'female')\n",
    "df.gender = df.gender.replace('cis male', 'male')\n",
    "df.gender = df.gender.replace('male.', 'male')\n",
    "df.gender = df.gender.replace('androgynous', 'nonbinary')\n",
    "df.gender = df.gender.replace('male 9:1 female roughly', 'nonbinary')\n",
    "df.gender = df.gender.replace('male (cis)', 'male')\n",
    "df.gender = df.gender.replace('other', 'nonbinary')\n",
    "df.gender = df.gender.replace('nb masculine', 'nonbinary')\n",
    "df.gender = df.gender.replace('cisgender female', 'female')\n",
    "df.gender = df.gender.replace('sex is male', 'male')\n",
    "df.gender = df.gender.replace('none of your business', 'nonbinary')\n",
    "df.gender = df.gender.replace('genderqueer', 'nonbinary')\n",
    "df.gender = df.gender.replace('human', 'nonbinary')\n",
    "df.gender = df.gender.replace('genderfluid', 'nonbinary')\n",
    "df.gender = df.gender.replace('enby', 'nonbinary')\n",
    "df.gender = df.gender.replace('malr', 'male')\n",
    "df.gender = df.gender.replace('genderqueer woman', 'nonbinary')\n",
    "df.gender = df.gender.replace('mtf', 'female')\n",
    "df.gender = df.gender.replace('queer', 'nonbinary')\n",
    "df.gender = df.gender.replace('agender', 'nonbinary')\n",
    "df.gender = df.gender.replace('dude', 'male')\n",
    "df.gender = df.gender.replace('fluid', 'nonbinary')\n",
    "df.gender = df.gender.replace('im a man why didnt you make this a drop down question. you should of asked sex? and i would of answered yes please. seriously how much text can this take? ', 'male')\n",
    "df.gender = df.gender.replace('mail', 'male')\n",
    "df.gender = df.gender.replace('m|', 'male')\n",
    "df.gender = df.gender.replace('male/genderqueer', 'nonbinary')\n",
    "df.gender = df.gender.replace('fem', 'female')\n",
    "df.gender = df.gender.replace('nonbinary', 'nonbinary')\n",
    "df.gender = df.gender.replace('female (props for making this a freeform field though)', 'nonbinary')\n",
    "df.gender = df.gender.replace(' female', 'female')\n",
    "df.gender = df.gender.replace('unicorn', 'nonbinary')\n",
    "df.gender = df.gender.replace('male (trans ftm)', 'male')\n",
    "df.gender = df.gender.replace('cis-woman', 'female')\n",
    "df.gender = df.gender.replace('cisdude', 'male')\n",
    "df.gender = df.gender.replace('genderflux demi-girl', 'nonbinary')\n",
    "df.gender = df.gender.replace('female-bodied; no feelings about gender', 'nonbinary')\n",
    "df.gender = df.gender.replace('cis man', 'male')\n",
    "df.gender = df.gender.replace('afab', 'nonbinary')\n",
    "df.gender = df.gender.replace('cis man', 'male')\n",
    "df.gender = df.gender.replace('transgender woman', 'female')\n",
    "df.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where gender is not given and confirm\n",
    "df.dropna(subset=['gender'], inplace=True) \n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male         1054\n",
       "female        339\n",
       "nonbinary      33\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many respondents of each gender?\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['26-100', '6-25', nan, 'More than 1000', '100-500', '500-1000',\n",
       "       '1-5'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no_employees needs to be fixed - 6-25 and 1-5 were converted into dates in the original dataset\n",
    "df.no_employees = df.no_employees.replace('25-Jun', '6-25')\n",
    "df.no_employees = df.no_employees.replace('5-Jan', '1-5')\n",
    "df.no_employees.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where no_employees is not given and confirm\n",
    "df.dropna(subset=['no_employees'], inplace=True) \n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remaining variables are all categorical (forced into categories by the survey), so no wild entries \n",
    "\n",
    "# get rid of any rows missing state\n",
    "df.dropna(subset=['state'], inplace=True) \n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male         493\n",
       "female       199\n",
       "nonbinary     14\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many respondents of each gender now?\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since nonbinary only represent about 2% of the data, regrettably, I am dropping them for this analysis\n",
    "df=df[df.gender.str.contains(\"nonbinary\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop categories that are missing a significant amount of responses\n",
    "df.drop([\"primary_role\", \"coverage\", \"self_help\", \"disclose\", \"neg_disclose\", \"work_disclose\", \"neg_work_disclose\", \"productivity\", \"productivity_percent\", \"other_mhd_reveal\"], axis=1, inplace=True)\n",
    "\n",
    "# drop all \"previous employer\" categories - 10% did not answer or was not applicable\n",
    "df.drop([\"previous_employ\", \"previous_benefits\", \"previous_care_options\", \"previous_wellness_program\", \"previous_seek_help\", \"previous_anonymity\", \"previous_mental_health_consequence\", \"previous_phys_health_consequence\", \"previous_coworkers\", \"previous_supervisor\", \"previous_mental_vs_physical\", \"previous_obs_consequence\", \"previous_obs_consequence.1\"], axis=1, inplace=True)\n",
    "\n",
    "# drop remaining categories with missing data\n",
    "# interview_why were free answer options for why they wouldn't disclose mental or physical conditions in job interviews\n",
    "# prof conditions is largely redundant with past, current, and which disorder\n",
    "# I decided to analyze by state of residence, because it was rarely different from state_work, which was missing one value\n",
    "df.drop([\"care_options\", \"phys_health_interview_why\", \"mental_health_interview_why\", \"prof_conditions\", \"country_work\", \"state_work\"], axis=1, inplace=True)\n",
    "\n",
    "# drop self_employed - these respondents tended to not answer a lot of questions, again because many questions were not relevant to them\n",
    "df.drop([\"self_employed\"], axis=1, inplace=True)\n",
    "\n",
    "# drop many categories that are more about attitudes and openness than health history and access\n",
    "df.drop([\"tech_company\", \"mental_health_consequence\", \"phys_health_consequence\", \"coworkers\", \"supervisor\", \"mental_health_interview\", \"phys_health_interview\", \"mental_vs_physical\", \"obs_consequence\", \"hurt_career\", \"negative_view\", \"which_disorder\", \"maybe_disorder\", \"prof_diagnosed\", \"position\", \"disclose_FandF\", \"past_disorder\", \"treatment\", \"work_interfere_tx\", \"work_interfere_notx\"], axis=1, inplace=True)\n",
    "\n",
    "# drop country because all are from the US now\n",
    "df.drop([\"country\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 692 entries, 1 to 1431\n",
      "Data columns (total 12 columns):\n",
      "no_employees        692 non-null object\n",
      "benefits            692 non-null object\n",
      "wellness_program    692 non-null object\n",
      "seek_help           692 non-null object\n",
      "anonymity           692 non-null object\n",
      "leave               692 non-null object\n",
      "family_history      692 non-null object\n",
      "current_disorder    692 non-null object\n",
      "age                 692 non-null float64\n",
      "gender              692 non-null object\n",
      "state               692 non-null object\n",
      "remote              692 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of model building, metrics, and interpretation\n",
    "\n",
    "My outcome variable is current_disorder, a categorical variable with three levels (maybe, no, and yes). I first need to create dummy variables for all of the independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 692 entries, 1 to 1431\n",
      "Data columns (total 12 columns):\n",
      "no_employees        692 non-null object\n",
      "benefits            692 non-null object\n",
      "wellness_program    692 non-null object\n",
      "seek_help           692 non-null object\n",
      "anonymity           692 non-null object\n",
      "leave               692 non-null object\n",
      "family_history      692 non-null object\n",
      "current_disorder    692 non-null object\n",
      "age                 692 non-null float64\n",
      "gender              692 non-null object\n",
      "state               692 non-null object\n",
      "remote              692 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummies = []\n",
    "cols = ['no_employees', 'benefits', 'wellness_program', 'seek_help', 'anonymity', 'leave', 'family_history', 'gender', 'state', 'remote']\n",
    "for col in cols:\n",
    "    dummies.append(pd.get_dummies(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummies = pd.concat(dummies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, dummies),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I dropped the original columns that were no longer needed. I should have also dropped the first dummy in each variable, since it is not needed in the model. I will go back and do this when I clean this up for posting later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop original columns that are no longer needed\n",
    "df = df.drop(['no_employees', 'benefits', 'wellness_program', 'seek_help', 'anonymity', 'leave', 'family_history', 'gender', 'state', 'remote'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sklearn to build a logistic regression and obtain the coefficients\n",
    "\n",
    "I will be using scikit-learn (rather than statsmodels) to build all of my models, because it is easy to switch between them. I will start with multinomial logistic regression, defining y (the outcome) as \"current_disorder\" and X (the independent variables) as the nine other variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=df.drop(\"current_disorder\", axis=1)\n",
    "y=df[\"current_disorder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "cv_class = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=3)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg=LogisticRegression()\n",
    "lg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the coefficients for the model. However, in quickly putting this project together, I realized I am not completely clear on interpreting the coefficients and OR values that can be obtained from the models, particularly in a multinomial context. This is another area I need to clean up (determining the relative contributions of each predictor) before posting this analysis publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>-0.022552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-5</th>\n",
       "      <td>0.257513</td>\n",
       "      <td>0.138226</td>\n",
       "      <td>-0.439358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-500</th>\n",
       "      <td>-0.198720</td>\n",
       "      <td>-0.199614</td>\n",
       "      <td>0.275307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26-100</th>\n",
       "      <td>-0.453362</td>\n",
       "      <td>0.044051</td>\n",
       "      <td>0.227463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1000</th>\n",
       "      <td>-0.057728</td>\n",
       "      <td>0.077109</td>\n",
       "      <td>-0.118243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-25</th>\n",
       "      <td>0.182477</td>\n",
       "      <td>-0.204575</td>\n",
       "      <td>-0.068270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>More than 1000</th>\n",
       "      <td>-0.216587</td>\n",
       "      <td>-0.144941</td>\n",
       "      <td>0.211107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.326752</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>-0.485454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>-0.642042</td>\n",
       "      <td>0.357079</td>\n",
       "      <td>0.054856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not eligible for coverage / N/A</th>\n",
       "      <td>-0.235969</td>\n",
       "      <td>-0.202262</td>\n",
       "      <td>0.298596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.064852</td>\n",
       "      <td>-0.483962</td>\n",
       "      <td>0.220009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>-0.642965</td>\n",
       "      <td>0.437241</td>\n",
       "      <td>-0.151895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.217107</td>\n",
       "      <td>-0.544341</td>\n",
       "      <td>0.173760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>-0.060548</td>\n",
       "      <td>-0.182644</td>\n",
       "      <td>0.066141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>-0.197348</td>\n",
       "      <td>0.076602</td>\n",
       "      <td>-0.117661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>-0.097855</td>\n",
       "      <td>-0.112179</td>\n",
       "      <td>0.004710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>-0.191203</td>\n",
       "      <td>-0.254167</td>\n",
       "      <td>0.200957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>-0.028850</td>\n",
       "      <td>-0.037098</td>\n",
       "      <td>-0.122088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.194692</td>\n",
       "      <td>-0.475908</td>\n",
       "      <td>0.106441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>-0.652249</td>\n",
       "      <td>0.223263</td>\n",
       "      <td>0.103653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.199060</td>\n",
       "      <td>0.173834</td>\n",
       "      <td>-0.405149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neither easy nor difficult</th>\n",
       "      <td>0.067630</td>\n",
       "      <td>-0.107043</td>\n",
       "      <td>-0.029430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somewhat difficult</th>\n",
       "      <td>-0.253430</td>\n",
       "      <td>-0.224325</td>\n",
       "      <td>0.302012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somewhat easy</th>\n",
       "      <td>-0.054848</td>\n",
       "      <td>-0.064084</td>\n",
       "      <td>0.024715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very difficult</th>\n",
       "      <td>-0.321920</td>\n",
       "      <td>-0.336725</td>\n",
       "      <td>0.412427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very easy</th>\n",
       "      <td>-0.122898</td>\n",
       "      <td>0.268599</td>\n",
       "      <td>-0.216568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.461122</td>\n",
       "      <td>-0.490366</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>-0.631878</td>\n",
       "      <td>1.042482</td>\n",
       "      <td>-0.791117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>-0.315650</td>\n",
       "      <td>-0.841860</td>\n",
       "      <td>0.873483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>-0.444328</td>\n",
       "      <td>-0.277921</td>\n",
       "      <td>0.283362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>0.561940</td>\n",
       "      <td>-0.048820</td>\n",
       "      <td>-0.477492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>-0.349010</td>\n",
       "      <td>-0.137611</td>\n",
       "      <td>0.400566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>-0.491163</td>\n",
       "      <td>0.646284</td>\n",
       "      <td>-0.218531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>-0.260355</td>\n",
       "      <td>0.492695</td>\n",
       "      <td>-0.208464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>-0.354563</td>\n",
       "      <td>0.776852</td>\n",
       "      <td>-0.440035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>0.161276</td>\n",
       "      <td>0.204127</td>\n",
       "      <td>-0.386567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>-0.054854</td>\n",
       "      <td>0.258611</td>\n",
       "      <td>-0.264668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>0.105451</td>\n",
       "      <td>-0.572793</td>\n",
       "      <td>0.441753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>-0.065643</td>\n",
       "      <td>-0.195525</td>\n",
       "      <td>0.241103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>0.090756</td>\n",
       "      <td>-0.225063</td>\n",
       "      <td>0.136307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>-0.172289</td>\n",
       "      <td>-0.310644</td>\n",
       "      <td>0.487005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>0.681489</td>\n",
       "      <td>-0.737016</td>\n",
       "      <td>0.081254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>0.447988</td>\n",
       "      <td>-0.095169</td>\n",
       "      <td>-0.291522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>-0.604384</td>\n",
       "      <td>0.763914</td>\n",
       "      <td>-0.264283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>-0.565190</td>\n",
       "      <td>0.345203</td>\n",
       "      <td>0.138466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>-0.401592</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.234188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>-0.722460</td>\n",
       "      <td>0.826895</td>\n",
       "      <td>-0.199839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>-0.161292</td>\n",
       "      <td>-0.387380</td>\n",
       "      <td>0.573911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>0.109268</td>\n",
       "      <td>0.172958</td>\n",
       "      <td>-0.257760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>-0.125781</td>\n",
       "      <td>0.081750</td>\n",
       "      <td>0.040218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>0.646756</td>\n",
       "      <td>-0.618472</td>\n",
       "      <td>-0.019562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>0.659575</td>\n",
       "      <td>-0.948421</td>\n",
       "      <td>0.260579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>-0.438738</td>\n",
       "      <td>0.949756</td>\n",
       "      <td>-0.563155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>0.503840</td>\n",
       "      <td>-0.683775</td>\n",
       "      <td>0.159007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.454252</td>\n",
       "      <td>-0.289094</td>\n",
       "      <td>-0.098778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>0.405297</td>\n",
       "      <td>-0.126419</td>\n",
       "      <td>-0.340604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>-0.044915</td>\n",
       "      <td>-0.441435</td>\n",
       "      <td>0.425372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always</th>\n",
       "      <td>0.001268</td>\n",
       "      <td>-0.248070</td>\n",
       "      <td>0.021588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>0.019687</td>\n",
       "      <td>-0.088438</td>\n",
       "      <td>-0.093416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sometimes</th>\n",
       "      <td>-0.507361</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>0.159834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0         1         2\n",
       "age                              0.003716  0.018356 -0.022552\n",
       "1-5                              0.257513  0.138226 -0.439358\n",
       "100-500                         -0.198720 -0.199614  0.275307\n",
       "26-100                          -0.453362  0.044051  0.227463\n",
       "500-1000                        -0.057728  0.077109 -0.118243\n",
       "6-25                             0.182477 -0.204575 -0.068270\n",
       "More than 1000                  -0.216587 -0.144941  0.211107\n",
       "I don't know                     0.326752  0.039401 -0.485454\n",
       "No                              -0.642042  0.357079  0.054856\n",
       "Not eligible for coverage / N/A -0.235969 -0.202262  0.298596\n",
       "Yes                              0.064852 -0.483962  0.220009\n",
       "I don't know                    -0.642965  0.437241 -0.151895\n",
       "No                               0.217107 -0.544341  0.173760\n",
       "Yes                             -0.060548 -0.182644  0.066141\n",
       "I don't know                    -0.197348  0.076602 -0.117661\n",
       "No                              -0.097855 -0.112179  0.004710\n",
       "Yes                             -0.191203 -0.254167  0.200957\n",
       "I don't know                    -0.028850 -0.037098 -0.122088\n",
       "No                               0.194692 -0.475908  0.106441\n",
       "Yes                             -0.652249  0.223263  0.103653\n",
       "I don't know                     0.199060  0.173834 -0.405149\n",
       "Neither easy nor difficult       0.067630 -0.107043 -0.029430\n",
       "Somewhat difficult              -0.253430 -0.224325  0.302012\n",
       "Somewhat easy                   -0.054848 -0.064084  0.024715\n",
       "Very difficult                  -0.321920 -0.336725  0.412427\n",
       "Very easy                       -0.122898  0.268599 -0.216568\n",
       "I don't know                     0.461122 -0.490366  0.005640\n",
       "No                              -0.631878  1.042482 -0.791117\n",
       "Yes                             -0.315650 -0.841860  0.873483\n",
       "female                          -0.444328 -0.277921  0.283362\n",
       "...                                   ...       ...       ...\n",
       "Michigan                         0.561940 -0.048820 -0.477492\n",
       "Minnesota                       -0.349010 -0.137611  0.400566\n",
       "Missouri                        -0.491163  0.646284 -0.218531\n",
       "Montana                         -0.260355  0.492695 -0.208464\n",
       "Nebraska                        -0.354563  0.776852 -0.440035\n",
       "Nevada                           0.161276  0.204127 -0.386567\n",
       "New Hampshire                   -0.054854  0.258611 -0.264668\n",
       "New Jersey                       0.105451 -0.572793  0.441753\n",
       "New Mexico                      -0.065643 -0.195525  0.241103\n",
       "New York                         0.090756 -0.225063  0.136307\n",
       "North Carolina                  -0.172289 -0.310644  0.487005\n",
       "North Dakota                     0.681489 -0.737016  0.081254\n",
       "Ohio                             0.447988 -0.095169 -0.291522\n",
       "Oklahoma                        -0.604384  0.763914 -0.264283\n",
       "Oregon                          -0.565190  0.345203  0.138466\n",
       "Pennsylvania                    -0.401592  0.038597  0.234188\n",
       "Rhode Island                    -0.722460  0.826895 -0.199839\n",
       "South Carolina                  -0.161292 -0.387380  0.573911\n",
       "South Dakota                     0.109268  0.172958 -0.257760\n",
       "Tennessee                       -0.125781  0.081750  0.040218\n",
       "Texas                            0.646756 -0.618472 -0.019562\n",
       "Utah                             0.659575 -0.948421  0.260579\n",
       "Vermont                         -0.438738  0.949756 -0.563155\n",
       "Virginia                         0.503840 -0.683775  0.159007\n",
       "Washington                       0.454252 -0.289094 -0.098778\n",
       "West Virginia                    0.405297 -0.126419 -0.340604\n",
       "Wisconsin                       -0.044915 -0.441435  0.425372\n",
       "Always                           0.001268 -0.248070  0.021588\n",
       "Never                            0.019687 -0.088438 -0.093416\n",
       "Sometimes                       -0.507361  0.046765  0.159834\n",
       "\n",
       "[81 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lg.coef_.T, index=X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                1.018525\n",
       "1-5                                1.293708\n",
       "100-500                            1.316935\n",
       "26-100                             1.255411\n",
       "500-1000                           1.080159\n",
       "6-25                               1.200187\n",
       "More than 1000                     1.235044\n",
       "I don't know                       1.386458\n",
       "No                                 1.429149\n",
       "Not eligible for coverage / N/A    1.347964\n",
       "Yes                                1.246087\n",
       "I don't know                       1.548430\n",
       "No                                 1.242477\n",
       "Yes                                1.068378\n",
       "I don't know                       1.079612\n",
       "No                                 1.004721\n",
       "Yes                                1.222573\n",
       "I don't know                       0.971562\n",
       "No                                 1.214936\n",
       "Yes                                1.250149\n",
       "I don't know                       1.220255\n",
       "Neither easy nor difficult         1.069969\n",
       "Somewhat difficult                 1.352577\n",
       "Somewhat easy                      1.025023\n",
       "Very difficult                     1.510479\n",
       "Very easy                          1.308130\n",
       "I don't know                       1.585852\n",
       "No                                 2.836249\n",
       "Yes                                2.395240\n",
       "female                             1.327586\n",
       "male                               0.988247\n",
       "Alabama                            2.002989\n",
       "Alaska                             1.766746\n",
       "Arizona                            2.801022\n",
       "California                         1.212963\n",
       "Colorado                           2.252403\n",
       "Connecticut                        1.257215\n",
       "Delaware                           1.274892\n",
       "District of Columbia               1.616604\n",
       "Florida                            1.148807\n",
       "Georgia                            2.316161\n",
       "Idaho                              1.934931\n",
       "Illinois                           1.613167\n",
       "Indiana                            1.551905\n",
       "Iowa                               2.760708\n",
       "Kansas                             1.609871\n",
       "Kentucky                           1.414305\n",
       "Louisiana                          1.840999\n",
       "Maine                              1.363292\n",
       "Maryland                           1.739110\n",
       "Massachusetts                      1.418631\n",
       "Michigan                           1.754073\n",
       "Minnesota                          1.492669\n",
       "Missouri                           1.908436\n",
       "Montana                            1.636722\n",
       "Nebraska                           2.174616\n",
       "Nevada                             1.226454\n",
       "New Hampshire                      1.295129\n",
       "New Jersey                         1.555431\n",
       "New Mexico                         1.272652\n",
       "New York                           1.146034\n",
       "North Carolina                     1.627434\n",
       "North Dakota                       1.976820\n",
       "Ohio                               1.565160\n",
       "Oklahoma                           2.146661\n",
       "Oregon                             1.412276\n",
       "Pennsylvania                       1.263882\n",
       "Rhode Island                       2.286209\n",
       "South Carolina                     1.775197\n",
       "South Dakota                       1.188817\n",
       "Tennessee                          1.085184\n",
       "Texas                              1.909337\n",
       "Utah                               1.933971\n",
       "Vermont                            2.585080\n",
       "Virginia                           1.655064\n",
       "Washington                         1.574994\n",
       "West Virginia                      1.499748\n",
       "Wisconsin                          1.530160\n",
       "Always                             1.021823\n",
       "Never                              1.019882\n",
       "Sometimes                          1.173316\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "np.exp(pd.DataFrame(lg.coef_.T, index=X.columns.values).max(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance with cross-validation\n",
    "\n",
    "I performed cross-validation on the data with 5 folds and obtained the mean accuracy and its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.567876133876 0.0380672219029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "cv = KFold(len(df),n_folds=5, shuffle=True)\n",
    "perf = cross_val_score(lg, X, y, cv=cv, scoring=\"accuracy\")\n",
    "print perf.mean(), perf.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619942196532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_labels=np.array(df.current_disorder)\n",
    "predictions=np.array(lg.predict(X))\n",
    "print accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comparison to baseline performance/prevalence\n",
    "\n",
    "To set a baseline to compare model performance against, I calculated the accuracy we would have if we guessed every person in the data set said \"yes\" to to the question of if they have a current mental health disorder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "current_disorder\n",
       "Maybe    145\n",
       "No       231\n",
       "Yes      316\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline performance if we just guessed 1 for everyone\n",
    "counts = df.groupby('current_disorder').size()\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The baseline accuracy for \"Yes\" is 45.7%, and the accuracy of the logistic regression model is 62.0%, so the model is quite a bit better than guessing, but far from 100% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use grid search to tune the model\n",
    "\n",
    "I then used grid search to tune the model, which tests several different versions of the model using a matrix of different sets of parameters to find the one with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=692, n_folds=5, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [1e-06, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "lg = LogisticRegression()\n",
    "params = {\"C\": [0.000001, 0.001, 0.01, 0.1, 1, 10, 100],\"penalty\":[\"l1\",\"l2\"]}\n",
    "\n",
    "lg_grid = GridSearchCV(lg, params, n_jobs=-1, verbose=True, cv=cv)\n",
    "lg_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This process determined the best parameters to obtain the best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.1}\n",
      "0.583815028902\n"
     ]
    }
   ],
   "source": [
    "print lg_grid.best_params_\n",
    "print lg_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, accuracy did not improve. \n",
    "\n",
    "I was suprised that accuracy did not improve, because I used the entire dataset. It was suggested that because I had the shuffle option on for cross-validation, something happened with the formation of the folds such that I was not able to gain accuracy via grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.032113</td>\n",
       "      <td>-0.008715</td>\n",
       "      <td>-0.014667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-500</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26-100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>More than 1000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.334823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not eligible for coverage / N/A</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.265459</td>\n",
       "      <td>0.110552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.407851</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>-0.357328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neither easy nor difficult</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somewhat difficult</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somewhat easy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very difficult</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very easy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I don't know</th>\n",
       "      <td>0.317457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.049178</td>\n",
       "      <td>-0.532872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.522455</td>\n",
       "      <td>0.821086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>-0.037291</td>\n",
       "      <td>-0.033246</td>\n",
       "      <td>0.214206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.024897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Never</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sometimes</th>\n",
       "      <td>-0.248660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0         1         2\n",
       "age                             -0.032113 -0.008715 -0.014667\n",
       "1-5                              0.000000  0.000000  0.000000\n",
       "100-500                          0.000000  0.000000  0.000000\n",
       "26-100                           0.000000  0.000000  0.000000\n",
       "500-1000                         0.000000  0.000000  0.000000\n",
       "6-25                             0.000000  0.000000  0.000000\n",
       "More than 1000                   0.000000  0.000000  0.000000\n",
       "I don't know                     0.000000  0.000000 -0.334823\n",
       "No                               0.000000  0.000000  0.000000\n",
       "Not eligible for coverage / N/A  0.000000  0.000000  0.000000\n",
       "Yes                              0.000000 -0.265459  0.110552\n",
       "I don't know                     0.000000  0.000000  0.000000\n",
       "No                               0.000000 -0.407851  0.000000\n",
       "Yes                              0.000000  0.000000  0.000000\n",
       "I don't know                     0.000000  0.000000  0.000000\n",
       "No                               0.000000  0.000000  0.000000\n",
       "Yes                              0.000000  0.000000  0.000000\n",
       "I don't know                     0.000000  0.000000  0.000000\n",
       "No                               0.000000  0.000000  0.000000\n",
       "Yes                             -0.357328  0.000000  0.000000\n",
       "I don't know                     0.000000  0.000000  0.000000\n",
       "Neither easy nor difficult       0.000000  0.000000  0.000000\n",
       "Somewhat difficult               0.000000  0.000000  0.000000\n",
       "Somewhat easy                    0.000000  0.000000  0.000000\n",
       "Very difficult                   0.000000  0.000000  0.000000\n",
       "Very easy                        0.000000  0.000000  0.000000\n",
       "I don't know                     0.317457  0.000000  0.000000\n",
       "No                               0.000000  1.049178 -0.532872\n",
       "Yes                              0.000000 -0.522455  0.821086\n",
       "female                          -0.037291 -0.033246  0.214206\n",
       "male                             0.000000  0.000000 -0.024897\n",
       "Alabama                          0.000000  0.000000  0.000000\n",
       "Alaska                           0.000000  0.000000  0.000000\n",
       "Arizona                          0.000000  0.000000  0.000000\n",
       "California                       0.000000  0.000000  0.000000\n",
       "Colorado                         0.000000  0.000000  0.000000\n",
       "Connecticut                      0.000000  0.000000  0.000000\n",
       "Delaware                         0.000000  0.000000  0.000000\n",
       "District of Columbia             0.000000  0.000000  0.000000\n",
       "Florida                          0.000000  0.000000  0.000000\n",
       "Georgia                          0.000000  0.000000  0.000000\n",
       "Idaho                            0.000000  0.000000  0.000000\n",
       "Illinois                         0.000000  0.000000  0.000000\n",
       "Indiana                          0.000000  0.000000  0.000000\n",
       "Iowa                             0.000000  0.000000  0.000000\n",
       "Kansas                           0.000000  0.000000  0.000000\n",
       "Kentucky                         0.000000  0.000000  0.000000\n",
       "Louisiana                        0.000000  0.000000  0.000000\n",
       "Maine                            0.000000  0.000000  0.000000\n",
       "Maryland                         0.000000  0.000000  0.000000\n",
       "Massachusetts                    0.000000  0.000000  0.000000\n",
       "Michigan                         0.000000  0.000000  0.000000\n",
       "Minnesota                        0.000000  0.000000  0.000000\n",
       "Missouri                         0.000000  0.000000  0.000000\n",
       "Montana                          0.000000  0.000000  0.000000\n",
       "Nebraska                         0.000000  0.000000  0.000000\n",
       "Nevada                           0.000000  0.000000  0.000000\n",
       "New Hampshire                    0.000000  0.000000  0.000000\n",
       "New Jersey                       0.000000  0.000000  0.000000\n",
       "New Mexico                       0.000000  0.000000  0.000000\n",
       "New York                         0.000000  0.000000  0.000000\n",
       "North Carolina                   0.000000  0.000000  0.000000\n",
       "North Dakota                     0.000000  0.000000  0.000000\n",
       "Ohio                             0.000000  0.000000  0.000000\n",
       "Oklahoma                         0.000000  0.000000  0.000000\n",
       "Oregon                           0.000000  0.000000  0.000000\n",
       "Pennsylvania                     0.000000  0.000000  0.000000\n",
       "Rhode Island                     0.000000  0.000000  0.000000\n",
       "South Carolina                   0.000000  0.000000  0.000000\n",
       "South Dakota                     0.000000  0.000000  0.000000\n",
       "Tennessee                        0.000000  0.000000  0.000000\n",
       "Texas                            0.000000  0.000000  0.000000\n",
       "Utah                             0.000000  0.000000  0.000000\n",
       "Vermont                          0.000000  0.000000  0.000000\n",
       "Virginia                         0.000000  0.000000  0.000000\n",
       "Washington                       0.000000  0.000000  0.000000\n",
       "West Virginia                    0.000000  0.000000  0.000000\n",
       "Wisconsin                        0.000000  0.000000  0.000000\n",
       "Always                           0.000000  0.000000  0.000000\n",
       "Never                            0.000000  0.000000  0.000000\n",
       "Sometimes                       -0.248660  0.000000  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lg_grid.best_estimator_.coef_.T, index=X.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running the additional models below, logistic regression turned out to be the best model. I came back here to use a 70/30 train-test split to show accuracy, precision, recall for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.30,\n",
    "                                                        random_state=15, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591346153846\n",
      "0.591346153846\n",
      "0.464854031511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lg_grid.best_estimator_.fit(X_train, y_train)\n",
    "print accuracy_score(y_test, lg_grid.best_estimator_.predict(X_test))\n",
    "print recall_score(y_test, lg_grid.best_estimator_.predict(X_test))\n",
    "print precision_score(y_test, lg_grid.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I couldn't get the confusion matrix to work on my data. I used a crosstab of the counts in each category from above to make a sort-of-confusion matrix to show performance across categories. Actually, the model was not bad for yes and no, and performed the worst for \"maybe\". It might be interesting to re-run this analysis excluding the \"maybe\" responses and only looking at the yes and no responses as a binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Maybe</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Maybe</th>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>15</td>\n",
       "      <td>142</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  Maybe   No  Yes\n",
       "row_0                 \n",
       "Maybe     32   38   75\n",
       "No        15  142   74\n",
       "Yes       15   46  255"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although logistic regression performed the best, I also tried K Nearest Neighbors and Decision Tree for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "\n",
    "Building the model and evaluating mean performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424877489313 0.0263902481343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X,y)\n",
    "perf = cross_val_score(knn, X, y, cv=cv)\n",
    "print perf.mean(), perf.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search to tune the KNN model to see if it performed better than the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=692, n_folds=5, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [2, 4, 6, 8, 10, 12, 14, 16, 18]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "params = {\"n_neighbors\": range(2,20,2)}\n",
    "\n",
    "knn_grid = GridSearchCV(knn, params, n_jobs=-1, verbose=True, cv=cv)\n",
    "knn_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489884393064\n",
      "{'n_neighbors': 10}\n"
     ]
    }
   ],
   "source": [
    "print knn_grid.best_score_\n",
    "print knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs better than guessing (accuracy of 49.7% as opposed to 45.7% for guessing), but much worse than logistic regression (49.7% vs. 62%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Building the model and evaluating mean performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442216661453 0.0365749936239\n"
     ]
    }
   ],
   "source": [
    "perf = cross_val_score(dt, X, y, cv=cv)\n",
    "print perf.mean(), perf.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search to tune the DT model to see if it performed better than the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=692, n_folds=5, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [0.1, 0.3, 0.7, 1], 'min_samples_split': [2, 3, 5], 'max_depth': [3, 5, 7, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "params = {\"max_depth\": [3,5,7,10,20],\"max_features\":[0.1, 0.3, 0.7, 1], \\\n",
    "         \"min_samples_split\":[2,3,5]}\n",
    "\n",
    "dt_grid = GridSearchCV(dt, params, n_jobs=-1, verbose=True, cv=cv)\n",
    "dt_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=0.7, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.579479768786\n"
     ]
    }
   ],
   "source": [
    "print dt_grid.best_estimator_\n",
    "print dt_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs better than guessing (accuracy of 55.9% as opposed to 45.7% for guessing), but much worse than logistic regression (55.8% vs. 62.0%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "Using nine categorical variables, I tried three methods to predict the outcome variable current_disorder. The models I tried were logistic regression, K Nearest Neighbors, and Decision Tree. Logistic regression performed the best, although cross-validation and grid search failed to improve it. All three models performed above random guessing for predicting the outcome variable. Based on the coefficients of the logistic regression, family history, state, and availability of mental health care coverage may be the most influential variables in determining the outcome, although I need to study the interpretation of these variables more. \n",
    "\n",
    "The dataset includes much more information than just the ten variables explored here, so I am very interested in conducting fulture analyses on these data, particularly the international vs. US attitudes on mental health, in the future. \n",
    "\n",
    "*Special thanks to Phillippa for all of her help throughout the course, and her patience when I abandoned my earlier project at a late date. The dog shelter practice example was a very helpful guide for this analysis.*\n",
    "\n",
    "Completed Dec. 1, 2016"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
